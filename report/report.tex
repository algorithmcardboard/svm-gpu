\documentclass{article}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{amsmath}
\begin{document}
\title{Libsvm using GPU}
\author{Anirudhan J. Rajagopalan \\ N18824115 \\ ajr619}
\maketitle

% write about lib svm format (svm light)
% two possible places for optimizations.  
% Calculating the gram matrix
% Porting the Sequential Minimal Optimization to GPU.

% Structures in libsvm
% svm_problem
% svm_node

% lru cache for all the rows processed in the kernel
% svm.cpp dot function (line no. 294). Modifying the dot function is difficult 
% as the internal datastructures has to be changed.

\newpage

\section{Abstract}
This project is an attempt to implement the Support Vector Machine~\cite{cortes1995} classifier using GPUs.
We tried two approaches: 
The first one is to try to port the Sequential Minimal Optimization part to the GPU\@.
The second one is to implement the Gram matrix~\cite{wiki:gram_matrix} calculation to GPUs.
We end up using the second approach and we show show performance improvements over the conventional approach used by libsvm. 
\section{Introduction}
The most popular implementation of SVM is libsvm~\cite{CC01a}.  
It is used in almost all the traditional machine learning applications.
Since it projects data into a higher dimensional space, we were able to solve problems that are not easily classifiable in the lower dimensional space.

Porting a SVM to GPUs will help us use this classifier easily in deep learning models too.
Though deep learning already uses various non-linearity to support higher dimensional data, the use of SVM might help us learn the underlying representation without using deeper layers in a deep learning model.

In this project, we explore various ways to parallelize the SVM problem and implement it in GPU\@.


\section{Background Information}

SVM is a linear classifier and is mainly successful due to its use of Hilbert space.  
SVM uses kernel functions which are functions in Reproducing Kernel Hilbert Space (RKHS).  
Implementation wise, these kernel functions can be as simple as computing a dot product between two set of features.o
For the purpose of this project, we will be worrying about these simple RKHS functions.
The idea is that, a non-linear lower dimensional data might be separable in a higher dimensional space.

Once, the higher dimensional features are calculated, SVM uses this higher dimensional features to find a classifier that has maximal distance from any of the points from the dataset.  

The other part of SVM is the Quadratic Programming part.  
This part involves actually finding the plane that classifies the points in the dataset while also having a maximum margin from both the data points.

Given l examples $(\bar{x}_{1}, y_{1}), (\bar{x}_{2}, y_{2}), \ldots ,(\bar{x}_{l}, y_{l}), $ with $\bar{x}_{i} \in \mathbb{R}^{n}$ and $y_{i} \in \{-1, 1\} $ $\forall{i} $ where the regularization is controlled by $C$.

\begin{align}
    \min_{f \in H} C\sum_{i = 1}^{l} V(y_{i}, f(\bar{x}_{i})) + \frac{1}{2} {\vert\vert f \vert\vert}_{k}^{2}
\end{align}

Here V is the loss function which is typically Hinge loss.  
Usage of Hinge loss or any approximation of Hing loss such as Huber-Hinge loss helps us find a maximal margin plane that classifies the samples.

Typically we solve for the dual of this actual problem, which can be given as

\begin{align}
    \max_{\alpha \in \mathbb{R}^{l}} \sum_{i = 1}^{l} \alpha_{i} - \frac{1}{2} \alpha^{T}K\alpha
\end{align}

Subject to: $\sum_{i=1}^{l} y_{i}\alpha_{i} = 0$ and $0 \le \alpha_{i} \le C, $ $ i = 1,\ldots,l $ where $K_{ij} = y_{i}y{j}k(\bar{x}{i}, \bar{x}_{j})$ is a kernel function. Solving this requires quadratic programming approaches which gives us the classification function.

\begin{align}
    f(x) = \sum_{i=1}^{l} y_{i}\alpha_{i}k(\bar{x}, \bar{x}_{i}) + b
\end{align}

With all the theory mentioned as briefly as possible, we can proceed to find a way to implement this interesting problem using GPUs.

\section{Literature Survey}
Previous work in this problem and what are the pros and cons of each solution.
\section{Proposed solution}
\section{Experimental Setup}
Specs of the machine, problem size, etc.
\section{Experimental Result \& Analysis}
What are your findings. I expect much deeper description that ``As we can see X is better than Y''.
\section{Conclusions}
What are your main findings? Can you generalize them?

\bibliographystyle{plain}
\bibliography{references}

\end{document}
